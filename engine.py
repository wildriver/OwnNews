"""
Ranking Engine (åˆ†æ•£ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ç‰ˆ)
å…±æœ‰DB (articles) ã¨å€‹äººDB (user data) ã‚’åˆ†é›¢ã€‚
æƒ…å ±çš„å¥åº·ã‚¹ã‚³ã‚¢è¨ˆç®—æ©Ÿèƒ½ã‚’å«ã‚€ã€‚
"""

import json
import math
from collections import Counter

import numpy as np
from supabase import Client


def _parse_vector(v) -> list[float]:
    """Supabase pgvectorã®å€¤ã‚’floatãƒªã‚¹ãƒˆã«å¤‰æ›ã™ã‚‹ã€‚
    æ–‡å­—åˆ— "[0.01, -0.02, ...]" ã¾ãŸã¯ãƒªã‚¹ãƒˆã®ã©ã¡ã‚‰ã«ã‚‚å¯¾å¿œã€‚
    """
    if isinstance(v, str):
        return json.loads(v)
    return v


# ã‚ªãƒ³ãƒœãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ç”¨ã‚«ãƒ†ã‚´ãƒªå®šç¾©
ONBOARDING_CATEGORIES = [
    "æ”¿æ²»", "çµŒæ¸ˆ", "å›½éš›", "ITãƒ»ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼",
    "ã‚¹ãƒãƒ¼ãƒ„", "ã‚¨ãƒ³ã‚¿ãƒ¡", "ç§‘å­¦", "ç¤¾ä¼š", "åœ°æ–¹",
]


class RankingEngine:
    """å…±æœ‰DB + å€‹äººDB ã‚’ä½¿ã£ãŸè¨˜äº‹ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ™ã‚¯ãƒˆãƒ«ç®¡ç†ã€‚"""

    def __init__(
        self,
        articles_db: Client,
        user_db: Client,
        user_id: str = "default",
    ):
        self.articles_db = articles_db
        self.user_db = user_db
        self.user_id = user_id

    # --- ã‚ªãƒ³ãƒœãƒ¼ãƒ‡ã‚£ãƒ³ã‚° ---

    def is_onboarded(self) -> bool:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã‚ªãƒ³ãƒœãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ¸ˆã¿ã‹ã‚’è¿”ã™ã€‚"""
        resp = (
            self.user_db.table("user_profile")
            .select("onboarded")
            .limit(1)
            .execute()
        )
        if resp.data:
            return resp.data[0].get("onboarded", False)
        return False

    def complete_onboarding(
        self, liked_ids: list[str], disliked_ids: list[str]
    ) -> None:
        """ã‚ªãƒ³ãƒœãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’å®Œäº†ã—åˆæœŸãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆã™ã‚‹ã€‚"""
        # ğŸ‘è¨˜äº‹ã®embeddingã‚’å–å¾—
        if liked_ids:
            resp = (
                self.articles_db.table("articles")
                .select("embedding")
                .in_("id", liked_ids)
                .not_.is_("embedding", "null")
                .execute()
            )
            if resp.data:
                embeddings = np.array(
                    [_parse_vector(r["embedding"]) for r in resp.data],
                    dtype=np.float32,
                )
                # ğŸ‘è¨˜äº‹ã®embeddingã‚‚å–å¾—ã—ã¦è² ã®å½±éŸ¿ã‚’ä¸ãˆã‚‹
                neg_embeddings = None
                if disliked_ids:
                    neg_resp = (
                        self.articles_db.table("articles")
                        .select("embedding")
                        .in_("id", disliked_ids)
                        .not_.is_("embedding", "null")
                        .execute()
                    )
                    if neg_resp.data:
                        neg_embeddings = np.array(
                            [_parse_vector(r["embedding"]) for r in neg_resp.data],
                            dtype=np.float32,
                        )

                # åˆæœŸãƒ™ã‚¯ãƒˆãƒ« = ğŸ‘å¹³å‡ - 0.3 * ğŸ‘å¹³å‡
                avg = embeddings.mean(axis=0)
                if neg_embeddings is not None:
                    neg_avg = neg_embeddings.mean(axis=0)
                    avg = avg - 0.3 * neg_avg
                    norm = np.linalg.norm(avg)
                    if norm > 0:
                        avg = avg * (np.linalg.norm(embeddings.mean(axis=0)) / norm)

                self._save_user_vector(avg.tolist())

        # ã‚ªãƒ³ãƒœãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å®Œäº†ãƒ•ãƒ©ã‚°
        self.user_db.table("user_profile").update(
            {"onboarded": True}
        ).execute()

    def get_onboarding_articles(
        self, categories: list[str], count: int = 20
    ) -> list[dict]:
        """ã‚ªãƒ³ãƒœãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ç”¨ã®ä»£è¡¨è¨˜äº‹ã‚’å–å¾—ã™ã‚‹ã€‚"""
        results = []
        per_cat = max(3, count // max(1, len(categories)))
        for cat in categories:
            resp = (
                self.articles_db.table("articles")
                .select("id, title, link, summary, category, image_url")
                .ilike("category", f"%{cat}%")
                .not_.is_("embedding", "null")
                .limit(per_cat)
                .execute()
            )
            results.extend(resp.data or [])
        # ã‚«ãƒ†ã‚´ãƒªã§å–ã‚Œãªã„å ´åˆã¯ãƒ©ãƒ³ãƒ€ãƒ è£œå®Œ
        if len(results) < count:
            random_resp = self.articles_db.rpc(
                "random_articles", {"pick_count": count - len(results) + 5}
            ).execute()
            existing_ids = {r["id"] for r in results}
            for r in random_resp.data or []:
                if r["id"] not in existing_ids and len(results) < count:
                    results.append(r)
        return results[:count]

    # --- ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ™ã‚¯ãƒˆãƒ« ---

    def get_user_vector(self) -> list[float] | None:
        """å€‹äººDBã‹ã‚‰ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—ã™ã‚‹ã€‚"""
        resp = (
            self.user_db.table("user_vectors")
            .select("vector")
            .eq("user_id", self.user_id)
            .execute()
        )
        if resp.data:
            return _parse_vector(resp.data[0]["vector"])
        return None

    def _save_user_vector(self, vector: list[float]) -> None:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ™ã‚¯ãƒˆãƒ«ã‚’å€‹äººDBã«ä¿å­˜ã™ã‚‹ã€‚"""
        self.user_db.table("user_vectors").upsert({
            "user_id": self.user_id,
            "vector": vector,
        }).execute()

    def _init_user_vector(self) -> list[float]:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ™ã‚¯ãƒˆãƒ«ãŒæœªè¨­å®šã®å ´åˆã€æœ€æ–°è¨˜äº‹ã®å¹³å‡ãƒ™ã‚¯ãƒˆãƒ«ã§åˆæœŸåŒ–ã™ã‚‹ã€‚"""
        resp = (
            self.articles_db.table("articles")
            .select("embedding")
            .not_.is_("embedding", "null")
            .limit(100)
            .execute()
        )
        if not resp.data:
            return []
        embeddings = np.array(
            [_parse_vector(r["embedding"]) for r in resp.data],
            dtype=np.float32,
        )
        avg = embeddings.mean(axis=0).tolist()
        self._save_user_vector(avg)
        return avg

    # --- ãƒ©ãƒ³ã‚­ãƒ³ã‚° ---

    def rank(
        self, filter_strength: float = 0.5, top_n: int = 30
    ) -> list[dict]:
        """è¨˜äº‹ã‚’ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã—ã¦è¿”ã™ã€‚"""
        user_vec = self.get_user_vector()
        if not user_vec:
            user_vec = self._init_user_vector()
        if not user_vec:
            return self._get_latest(top_n)

        similar_count = max(1, int(top_n * filter_strength))
        random_count = top_n - similar_count

        # é¡ä¼¼åº¦ä¸Šä½ã‚’å–å¾—ï¼ˆå…±æœ‰DBï¼‰
        similar_resp = self.articles_db.rpc(
            "match_articles",
            {"query_vector": user_vec, "match_count": similar_count},
        ).execute()
        results = similar_resp.data or []

        # ãƒ©ãƒ³ãƒ€ãƒ è¨˜äº‹ã‚’å–å¾—ï¼ˆå…±æœ‰DBï¼‰
        if random_count > 0:
            similar_ids = {r["id"] for r in results}
            random_resp = self.articles_db.rpc(
                "random_articles",
                {"pick_count": random_count + 10},
            ).execute()
            for r in random_resp.data or []:
                if r["id"] not in similar_ids and len(results) < top_n:
                    r["similarity"] = 0.0
                    results.append(r)

        return results

    def _get_latest(self, limit: int) -> list[dict]:
        """ãƒ™ã‚¯ãƒˆãƒ«æœªè¨­å®šæ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æœ€æ–°è¨˜äº‹ã‚’è¿”ã™ã€‚"""
        resp = (
            self.articles_db.table("articles")
            .select("id, title, link, summary, published, category, image_url")
            .order("collected_at", desc=True)
            .limit(limit)
            .execute()
        )
        for r in resp.data:
            r["similarity"] = 0.0
        return resp.data

    # --- ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³è¨˜éŒ²ï¼ˆå€‹äººDBï¼‰ ---

    def _record_interaction(
        self, article_id: str, interaction_type: str
    ) -> None:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ“ä½œã‚’å€‹äººDBã®user_interactionsãƒ†ãƒ¼ãƒ–ãƒ«ã«è¨˜éŒ²ã™ã‚‹ã€‚"""
        self.user_db.table("user_interactions").upsert(
            {
                "user_id": self.user_id,
                "article_id": article_id,
                "interaction_type": interaction_type,
            },
            on_conflict="user_id,article_id,interaction_type",
        ).execute()

    def get_interacted_ids(
        self, interaction_types: list[str] | None = None
    ) -> set[str]:
        """æŒ‡å®šã‚¿ã‚¤ãƒ—ã®ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³æ¸ˆã¿ article_id ã‚’è¿”ã™ã€‚"""
        query = (
            self.user_db.table("user_interactions")
            .select("article_id")
            .eq("user_id", self.user_id)
        )
        if interaction_types:
            query = query.in_("interaction_type", interaction_types)
        resp = query.execute()
        return {r["article_id"] for r in resp.data}

    def get_interaction_history(
        self, interaction_types: list[str], limit: int = 50
    ) -> list[dict]:
        """ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³å±¥æ­´ã‚’è¨˜äº‹æƒ…å ±ä»˜ãã§è¿”ã™ã€‚"""
        resp = (
            self.user_db.table("user_interactions")
            .select("article_id, interaction_type, created_at")
            .eq("user_id", self.user_id)
            .in_("interaction_type", interaction_types)
            .order("created_at", desc=True)
            .limit(limit)
            .execute()
        )
        if not resp.data:
            return []

        # è¨˜äº‹æƒ…å ±ã‚’å…±æœ‰DBã‹ã‚‰ä¸€æ‹¬å–å¾—
        article_ids = list({r["article_id"] for r in resp.data})
        articles_resp = (
            self.articles_db.table("articles")
            .select("id, title, link, category, published, image_url")
            .in_("id", article_ids)
            .execute()
        )
        article_map = {a["id"]: a for a in articles_resp.data}

        result = []
        for r in resp.data:
            article = article_map.get(r["article_id"], {})
            result.append({
                "article_id": r["article_id"],
                "interaction_type": r["interaction_type"],
                "created_at": r["created_at"],
                "title": article.get("title", "(å‰Šé™¤æ¸ˆã¿)"),
                "link": article.get("link", ""),
                "category": article.get("category", ""),
                "published": article.get("published", ""),
                "image_url": article.get("image_url", ""),
            })
        return result

    def get_stats(self) -> dict:
        """ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ç”¨ã®çµ±è¨ˆæƒ…å ±ã‚’è¿”ã™ã€‚"""
        # ç·è¨˜äº‹æ•°ï¼ˆå…±æœ‰DBï¼‰
        total_resp = (
            self.articles_db.table("articles")
            .select("id", count="exact")
            .execute()
        )
        total_articles = total_resp.count or 0

        # ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ä¸€è¦§ï¼ˆå€‹äººDBï¼‰
        interactions_resp = (
            self.user_db.table("user_interactions")
            .select("article_id, interaction_type")
            .eq("user_id", self.user_id)
            .execute()
        )
        interactions = interactions_resp.data or []
        view_count = sum(
            1 for i in interactions
            if i["interaction_type"] in ("view", "deep_dive")
        )
        not_interested_count = sum(
            1 for i in interactions
            if i["interaction_type"] == "not_interested"
        )

        # ã‚«ãƒ†ã‚´ãƒªåˆ¥é–²è¦§æ•°
        viewed_ids = [
            i["article_id"] for i in interactions
            if i["interaction_type"] in ("view", "deep_dive")
        ]
        category_counts: dict[str, int] = {}
        if viewed_ids:
            cat_resp = (
                self.articles_db.table("articles")
                .select("category")
                .in_("id", viewed_ids)
                .execute()
            )
            cats = [
                r["category"] for r in cat_resp.data
                if r.get("category")
            ]
            all_cats = []
            for c in cats:
                all_cats.extend(
                    t.strip() for t in c.split(",") if t.strip()
                )
            category_counts = dict(Counter(all_cats))

        # æ—¥åˆ¥åé›†æ•°ï¼ˆå…±æœ‰DBï¼‰
        daily_resp = (
            self.articles_db.table("articles")
            .select("collected_at")
            .order("collected_at", desc=True)
            .limit(2000)
            .execute()
        )
        daily_counts: dict[str, int] = {}
        for r in daily_resp.data:
            if r.get("collected_at"):
                day = r["collected_at"][:10]
                daily_counts[day] = daily_counts.get(day, 0) + 1

        return {
            "total_articles": total_articles,
            "view_count": view_count,
            "not_interested_count": not_interested_count,
            "category_counts": category_counts,
            "daily_counts": daily_counts,
        }

    # --- æƒ…å ±çš„å¥åº· ---

    def get_info_health(self) -> dict:
        """æƒ…å ±çš„å¥åº·ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ã™ã‚‹ã€‚

        é£Ÿäº‹ã®æ „é¤Šãƒãƒ©ãƒ³ã‚¹ã®ã‚¢ãƒŠãƒ­ã‚¸ãƒ¼ã§ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹æƒ…å ±ã®æ‘‚å–ãƒãƒ©ãƒ³ã‚¹ã‚’è©•ä¾¡ã€‚
        Shannon entropy ã§å¤šæ§˜æ€§ã‚’ã€æœ€é »ã‚«ãƒ†ã‚´ãƒªå æœ‰ç‡ã§åé£Ÿåº¦ã‚’æ¸¬å®šã™ã‚‹ã€‚
        """
        # é–²è¦§è¨˜äº‹ã®ã‚«ãƒ†ã‚´ãƒªåˆ†å¸ƒã‚’é›†è¨ˆï¼ˆå€‹äººDB + å…±æœ‰DBï¼‰
        interactions_resp = (
            self.user_db.table("user_interactions")
            .select("article_id, interaction_type")
            .eq("user_id", self.user_id)
            .in_("interaction_type", ["view", "deep_dive"])
            .execute()
        )
        viewed_ids = [r["article_id"] for r in (interactions_resp.data or [])]

        if not viewed_ids:
            return {
                "category_distribution": {},
                "diversity_score": 0,
                "dominant_category": "",
                "dominant_ratio": 0.0,
                "bias_level": "ãƒ‡ãƒ¼ã‚¿ä¸è¶³",
                "missing_categories": list(ONBOARDING_CATEGORIES),
                "total_viewed": 0,
            }

        # ã‚«ãƒ†ã‚´ãƒªæƒ…å ±ã‚’å…±æœ‰DBã‹ã‚‰å–å¾—
        cat_resp = (
            self.articles_db.table("articles")
            .select("category")
            .in_("id", viewed_ids)
            .execute()
        )
        all_cats: list[str] = []
        for r in cat_resp.data or []:
            if r.get("category"):
                all_cats.extend(
                    t.strip() for t in r["category"].split(",") if t.strip()
                )

        if not all_cats:
            return {
                "category_distribution": {},
                "diversity_score": 0,
                "dominant_category": "",
                "dominant_ratio": 0.0,
                "bias_level": "ãƒ‡ãƒ¼ã‚¿ä¸è¶³",
                "missing_categories": list(ONBOARDING_CATEGORIES),
                "total_viewed": len(viewed_ids),
            }

        # ã‚«ãƒ†ã‚´ãƒªåˆ†å¸ƒ
        counter = Counter(all_cats)
        total = sum(counter.values())
        distribution = dict(counter.most_common())

        # Shannon entropy ã§å¤šæ§˜æ€§ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ï¼ˆ0-100ã«æ­£è¦åŒ–ï¼‰
        n_categories = len(counter)
        if n_categories <= 1:
            diversity_score = 0
        else:
            entropy = -sum(
                (c / total) * math.log2(c / total)
                for c in counter.values()
            )
            max_entropy = math.log2(n_categories)
            diversity_score = int((entropy / max_entropy) * 100)

        # åé£Ÿåº¦ï¼ˆæœ€é »ã‚«ãƒ†ã‚´ãƒªã®å æœ‰ç‡ï¼‰
        dominant_cat, dominant_count = counter.most_common(1)[0]
        dominant_ratio = dominant_count / total

        if dominant_ratio > 0.6:
            bias_level = "åé£Ÿï¼ˆå¼·ï¼‰"
        elif dominant_ratio > 0.4:
            bias_level = "ã‚„ã‚„åã‚Š"
        else:
            bias_level = "ãƒãƒ©ãƒ³ã‚¹è‰¯å¥½"

        # ä¸è¶³ã‚«ãƒ†ã‚´ãƒªï¼ˆé–²è¦§æ•°ãŒ0ã¾ãŸã¯æ¥µç«¯ã«å°‘ãªã„ã‚«ãƒ†ã‚´ãƒªï¼‰
        seen_cats = set(counter.keys())
        missing = [
            c for c in ONBOARDING_CATEGORIES
            if c not in seen_cats
        ]

        return {
            "category_distribution": distribution,
            "diversity_score": diversity_score,
            "dominant_category": dominant_cat,
            "dominant_ratio": round(dominant_ratio, 2),
            "bias_level": bias_level,
            "missing_categories": missing,
            "total_viewed": len(viewed_ids),
        }

    # --- ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ ---

    def _get_article_embedding(self, article_id: str) -> np.ndarray | None:
        """è¨˜äº‹ã®embeddingãƒ™ã‚¯ãƒˆãƒ«ã‚’å…±æœ‰DBã‹ã‚‰å–å¾—ã™ã‚‹ã€‚"""
        resp = (
            self.articles_db.table("articles")
            .select("embedding")
            .eq("id", article_id)
            .execute()
        )
        if not resp.data or resp.data[0]["embedding"] is None:
            return None
        return np.array(
            _parse_vector(resp.data[0]["embedding"]), dtype=np.float32
        )

    def record_view(self, article_id: str) -> None:
        """è¨˜äº‹ã‚’é–²è¦§ã—ãŸ: å¼±ã„æ­£ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ (alpha=0.03)"""
        self._record_interaction(article_id, "view")
        self._apply_feedback(article_id, alpha=0.03)

    def record_deep_dive(self, article_id: str) -> None:
        """æ·±æ˜ã‚Šãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ãŸ: å¼·ã„æ­£ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ (alpha=0.15)"""
        self._record_interaction(article_id, "deep_dive")
        self._apply_feedback(article_id, alpha=0.15)

    def record_not_interested(self, article_id: str) -> None:
        """èˆˆå‘³ãªã—ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ãŸ: å¼·ã„è² ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ (alpha=-0.2)"""
        self._record_interaction(article_id, "not_interested")
        self._apply_feedback(article_id, alpha=-0.2)

    def _apply_feedback(self, article_id: str, alpha: float) -> None:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ™ã‚¯ãƒˆãƒ«ã‚’æ›´æ–°ã™ã‚‹ã€‚"""
        v = self._get_article_embedding(article_id)
        if v is None:
            return

        user_vec = self.get_user_vector()
        if user_vec is None:
            if alpha > 0:
                self._save_user_vector(v.tolist())
            return

        u = np.array(user_vec, dtype=np.float32)

        if alpha >= 0:
            new_vec = (1 - alpha) * u + alpha * v
        else:
            strength = abs(alpha)
            new_vec = (1 + strength) * u - strength * v
            norm = np.linalg.norm(new_vec)
            if norm > 0:
                new_vec = new_vec * (np.linalg.norm(u) / norm)

        self._save_user_vector(new_vec.tolist())
